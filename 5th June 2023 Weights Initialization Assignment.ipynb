{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897a212c-10ca-436c-9d79-b2181f9eb83d",
   "metadata": {},
   "source": [
    "# **Part 1: Upderstanding Weight Ipitialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1e0dd-12a7-46eb-a883-a8dd7cb29b98",
   "metadata": {},
   "source": [
    "## 1. Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize the weights carefully?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cc97e-3c44-4e89-974b-5a8af0756e73",
   "metadata": {},
   "source": [
    "Weights initialization is crucial step in training artificial neural network because it significantly affects the model's ability to learn something and converge to a optimal solution.\n",
    "\n",
    "* **Reasons why carefull weight initialization is important**\n",
    "\n",
    "1. **Avoiding Symmetry** : If all weights are initialized with same value so every neuron will return the output and also will get the same gradient by backpropagation. Because of it model will learn same thing instead of learning valuable patters.\n",
    "\n",
    "2. **Faster Convergence** : Proper initialized weights can help the gradient to flow in network easily, which can faster the training process.\n",
    "\n",
    "3. **Vanishing and Exploding Gradient** : Improper initialization can lead to either vanishing gradient or exploding gradient. Proper initialization method can the scale of gradient in range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643892d5-033a-47f9-8b0e-7282f4680987",
   "metadata": {},
   "source": [
    "## 2. Describe the challenges associated with improper weight initialization. How do these issues affect model training and convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb3d4e-2a27-4852-b1f2-eb95688196d7",
   "metadata": {},
   "source": [
    "* **Challenges**\n",
    "\n",
    "1. **Symmetry Problem** :If all weights are initialized to the same value, all neurons in a given layer will produce the same output and receive the same gradients during backpropagation. This means they will update in the same way, effectively making them identical and preventing the network from learning diverse features.\n",
    "\n",
    "2. **Vanishing Gradient** : If we set the value of weights too small so gradient can shrink as they are propagated bachward through the network. This is perticularly problematic for model where gradient exponentially decay, it may cause the slow convergence.\n",
    "\n",
    "3. **Exploding Gradient** : With the large value of weights, gradient can exponentially grow during backpropagation. This can cause the update of weights to become too large, which can lead unstable learning process.\n",
    "\n",
    "4. **Slove Convergence** :Improper initialization can lead to a poor starting point for the optimization algorithm. \n",
    "\n",
    "* **How do these issues affect Model Training and Convergence**\n",
    "\n",
    "1. **Training** : Poor weights initialization can lead the unstable training process, where the loss function doesn't decreases smoothly.\n",
    "\n",
    "2. **Layer activation Distribution** : Improper weights initialization make the layers saturating, which hampers effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3890ef-5ee4-45a5-b706-25030b6cd39d",
   "metadata": {},
   "source": [
    "## 3. Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the variance of weights during initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d5a69-f135-46bb-899e-5857939fdbfd",
   "metadata": {},
   "source": [
    "Variance in the context of weight initialization refers the deviation of weight values.\n",
    "\n",
    "* **Common Initialization methods considering Variance**\n",
    "\n",
    "1. Xavier/Glorot Initializer : This method is designed to keep the variance of activation consistant across the layers.\n",
    "\n",
    "It is suited for tanh() activation function.\n",
    "\n",
    "For a layer with N1(Input Unit) and N2(Output Unit) weights W are initialized as :\n",
    "\n",
    "W ∼ N(0,2 / N1+N2) # For normal distribution\n",
    "\n",
    "w ∼ U(−np.sqrt(6/N1+N2),np.sqrt(6/N1+N2))\n",
    "\n",
    "2. He Initialization : it is suited for ReLU() activation function.\n",
    "\n",
    "W ∼ N(0,2/N1)\n",
    "\n",
    "W ∼ U(−np.sqrt(6/N1),np.sqrt(6/N1))\n",
    "\n",
    "* **Importance of Variance in Weights Initialization**\n",
    "\n",
    "1. **Stability of activation** : If the variance is too high or too low, the activations can become unstable, either saturating the activation function (if too high) or producing near-zero values (if too low). This stability is essential for effective learning.\n",
    "\n",
    "2. **Gradient Flow** : If the variance is too high, gradients can explode, leading to large updates and instability. If the variance is too low, gradients can vanish, resulting in very small updates and slow convergence. Ensuring the right variance helps maintain a balance in gradient flow, which is critical for the efficient training of deep networks.\n",
    "\n",
    "3. **Avoiding Vanishing and Exploding Gradients** : As mentioned, appropriate variance helps prevent the vanishing and exploding gradient problems. This is particularly important for deep networks where gradients are propagated back through many layers, and even small deviations can amplify over many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5347af9-3bac-42d6-a364-32523087f8a0",
   "metadata": {},
   "source": [
    "# **Part 2: Weight Initialization Technique**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6f5c6-9673-4fb9-a593-310e33ce5dbf",
   "metadata": {},
   "source": [
    "## 4. Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d646e9-f695-4b44-b72d-6ffee684f3cb",
   "metadata": {},
   "source": [
    "* Zero initialization refers to initializing all the weights in a neural network to zero.\n",
    "\n",
    "* **Limitations**\n",
    "\n",
    "1. Symmetry Problem : When weights are initialized to zero, each neuron in a layer will compute the same output for the same input and receive the same gradient during backpropagation.\n",
    "\n",
    "2. Slow Convergence : Zero initialization does not provide a diverse starting point for the weights, which is necessary for the optimization algorithm to explore different regions of the loss landscape.\n",
    "\n",
    "3. Inability to Learn : In deeper networks, zero initialization can make it nearly impossible for the network to learn anything beyond the simplest patterns.\n",
    "\n",
    "* **When zero initialization might be appropriate**\n",
    "\n",
    "1. Bias Initialization : It is common and often beneficial to initialize the biases to zero. Biases do not suffer from the symmetry problem because they are not involved in the linear combination of inputs and weights. Instead, they provide a shift in the activation function, which can help the network learn more effectively.\n",
    "\n",
    "2. Debugging and Simplicity : Zero initialization can be used for debugging purposes to test other components of the network or training process without the influence of initial weight values. It can also be used as a simple baseline to compare against more sophisticated initialization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8218b-bd50-453d-b711-d55fb53f7128",
   "metadata": {},
   "source": [
    "## 5. Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614327e5-5b9a-49c4-ae78-fd07869746f3",
   "metadata": {},
   "source": [
    "* Random initialization involves assigning random values to the weights of a neural network. \n",
    "\n",
    "* **Adjustments to Mitigate Issues**\n",
    "\n",
    "1. Xavier/Glorot Initialization : This method is designed to maintain the variance of activations consistent across layers. It helps in keep the scale of gradient same across the layer.\n",
    "\n",
    "2. He Initialization : Particularly suited for layers with ReLU or similar activation functions.This initialization helps maintain a variance that prevents gradients from vanishing or exploding, which is critical for deep networks with ReLU activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92563ca-54a3-4dbd-bee8-8b5d4de70b86",
   "metadata": {},
   "source": [
    "## 6. Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e2e3b-7175-4c42-9022-cf985f756a97",
   "metadata": {},
   "source": [
    "* **Concept**\n",
    "\n",
    "Xavier/Glorot initialization sets the weights of a neural network based on the number of input and output units of each layer. The goal is to keep the scale of the gradients roughly the same across layers, preventing them from vanishing or exploding as they propagate through the network.\n",
    "\n",
    "* **Implementation**\n",
    "\n",
    "W ∼ N(0,2 / N1+N2) # For normal distribution\n",
    "\n",
    "w ∼ U(−np.sqrt(6/N1+N2),np.sqrt(6/N1+N2))\n",
    "\n",
    "* **Addressing chellenges of Improper weights Initialization**\n",
    "\n",
    "1. Vanishing Gradients : In deep networks, gradients can diminish as they propagate backward through each layer, especially when weights are initialized with very small values.\n",
    "\n",
    "2. Vanishing Gradients : In deep networks, gradients can diminish as they propagate backward through each layer, especially when weights are initialized with very small values.\n",
    "\n",
    "* **Underlying Theory**\n",
    "\n",
    "The theory behind Xavier/Glorot initialization is rooted in maintaining the variance of activations and gradients across layers. This is achieved by ensuring that the weights are scaled appropriately based on the size of the previous and next layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf612a-eab5-4746-a177-3a5f74b7d77c",
   "metadata": {},
   "source": [
    "## 7. Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c63dc-658c-4961-97ec-ad3f1d8d3f42",
   "metadata": {},
   "source": [
    "* **Concept**\n",
    "\n",
    "He initialization sets the weights of a neural network based on the number of input units in each layer. It aims to maintain the variance of the activations consistent across layers, particularly for ReLU activations, which are known to cause activation values to be zero for half of the inputs on average.\n",
    "\n",
    "* **Implementation**\n",
    "\n",
    "W ∼ N(0,2/N1)\n",
    "\n",
    "W ∼ U(−np.sqrt(6/N1),np.sqrt(6/N1))\n",
    "\n",
    "* **Differences from Xavier Initialization**\n",
    "\n",
    "1. Scaling Factor\n",
    "\n",
    "Xavier Initialization : Scales the weights based on both number of input and output units.\n",
    " \n",
    "He Initialization : Scales the weight by only input units.\n",
    "\n",
    "2. Target Activation Function\n",
    "\n",
    "Xavier Initialization : Suitable for tanh() activatioin function.\n",
    "\n",
    "He Initialization : Suitable for ReLU() activation function.\n",
    "\n",
    "* **When is it preffered**\n",
    "\n",
    "Used when we have ReLU() activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97983b2f-2a16-4914-bda6-75661ae7d431",
   "metadata": {},
   "source": [
    "# **Part 3 : Applying Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2fd31a-b3ff-4035-b534-95accf7b0b83",
   "metadata": {},
   "source": [
    "## 8. Implement different weight initialization techniques (zero initialization, random initialization, Xavier initialization, and He initialization) in a neural network using a framework of Eour choice. Train the model on a suitable dataset and compare the performance of the initialized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54aeed1-c257-4946-bf99-a28075d9cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e933dc0-6a35-495d-b36b-d958ecb783fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302ddc0e-c1e5-461d-82ef-6883dd535f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbec7f7-b774-4e32-8218-2b05db165a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid,x_train = x_train[:5000]/255,x_train[5000:]/255\n",
    "x_test = x_test/255\n",
    "y_valid,y_train = y_train[:5000],y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7242de1-33c7-467c-99f1-f8a19e7d2113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f028ed0-0662-4a13-939e-5e55b550ee83",
   "metadata": {},
   "source": [
    "**Zero Initializer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854ac4dc-f443-44af-af4d-0975a2b0539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(units=300,activation='relu',kernel_initializer=tf.zeros_initializer(),bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(units=110,activation='relu',kernel_initializer=tf.zeros_initializer(),bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(units=10,activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "751fe65c-ab11-4dfc-8cce-db459ee86a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zeros = tf.keras.models.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa278f94-7666-4e63-8b7f-e95ee8e4bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zeros.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d78c207-d87d-479a-995f-964ea2ba41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1091 - loss: 2.3019 - val_accuracy: 0.1126 - val_loss: 2.3011\n",
      "Epoch 2/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1112 - loss: 2.3013 - val_accuracy: 0.1126 - val_loss: 2.3011\n",
      "Epoch 3/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1126 - loss: 2.3013 - val_accuracy: 0.1126 - val_loss: 2.3010\n",
      "Epoch 4/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1126 - val_loss: 2.3011\n",
      "Epoch 5/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1147 - loss: 2.3009 - val_accuracy: 0.1126 - val_loss: 2.3007\n",
      "Epoch 6/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1116 - loss: 2.3012 - val_accuracy: 0.1126 - val_loss: 2.3010\n",
      "Epoch 7/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1136 - loss: 2.3012 - val_accuracy: 0.1126 - val_loss: 2.3013\n",
      "Epoch 8/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1157 - loss: 2.3010 - val_accuracy: 0.1126 - val_loss: 2.3010\n",
      "Epoch 9/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1131 - loss: 2.3013 - val_accuracy: 0.1126 - val_loss: 2.3007\n",
      "Epoch 10/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1102 - loss: 2.3016 - val_accuracy: 0.1126 - val_loss: 2.3009\n",
      "Epoch 11/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1100 - loss: 2.3016 - val_accuracy: 0.1126 - val_loss: 2.3010\n",
      "Epoch 12/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1130 - loss: 2.3013 - val_accuracy: 0.1126 - val_loss: 2.3008\n",
      "Epoch 13/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.1102 - loss: 2.3018 - val_accuracy: 0.1126 - val_loss: 2.3010\n",
      "Epoch 14/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1144 - loss: 2.3013 - val_accuracy: 0.1126 - val_loss: 2.3008\n",
      "Epoch 15/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1110 - loss: 2.3015 - val_accuracy: 0.1126 - val_loss: 2.3008\n",
      "Epoch 16/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1133 - loss: 2.3014 - val_accuracy: 0.1126 - val_loss: 2.3007\n",
      "Epoch 17/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1136 - loss: 2.3010 - val_accuracy: 0.1126 - val_loss: 2.3009\n",
      "Epoch 18/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1107 - loss: 2.3016 - val_accuracy: 0.1126 - val_loss: 2.3010\n",
      "Epoch 19/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1095 - loss: 2.3017 - val_accuracy: 0.1126 - val_loss: 2.3011\n",
      "Epoch 20/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1106 - loss: 2.3017 - val_accuracy: 0.1126 - val_loss: 2.3007\n"
     ]
    }
   ],
   "source": [
    "zero_initializer = model_zeros.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=20,batch_size=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11e074a3-7b09-4d3c-8066-503c0480064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1160 - loss: 2.3011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.301177501678467, 0.11349999904632568]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_zeros.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0445e83-460a-448e-a5c8-2e998f7990c9",
   "metadata": {},
   "source": [
    "**Random Initializer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c27049-6c18-4b92-9c6d-2fc7d288b2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "LAYERS_RANDOM = [\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(300,activation='relu',kernel_initializer=tf.random_normal_initializer(),bias_initializer=tf.random_normal_initializer()),\n",
    "    tf.keras.layers.Dense(110,activation='relu',kernel_initializer=tf.random_normal_initializer(),bias_initializer=tf.random_normal_initializer()),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaa42b7b-35a0-41eb-a7f9-81249deb41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random = tf.keras.models.Sequential(LAYERS_RANDOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41dd8292-3813-4e26-8131-2f0e524692af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8e9d8a0-458f-407a-acbc-b2e2cfd1b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8897 - loss: 0.3750 - val_accuracy: 0.9674 - val_loss: 0.1010\n",
      "Epoch 2/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0897 - val_accuracy: 0.9748 - val_loss: 0.0798\n",
      "Epoch 3/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0581 - val_accuracy: 0.9752 - val_loss: 0.0888\n",
      "Epoch 4/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9871 - loss: 0.0412 - val_accuracy: 0.9782 - val_loss: 0.0807\n",
      "Epoch 5/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9898 - loss: 0.0304 - val_accuracy: 0.9806 - val_loss: 0.0675\n",
      "Epoch 6/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9916 - loss: 0.0259 - val_accuracy: 0.9770 - val_loss: 0.0865\n",
      "Epoch 7/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9934 - loss: 0.0199 - val_accuracy: 0.9806 - val_loss: 0.0778\n",
      "Epoch 8/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9930 - loss: 0.0198 - val_accuracy: 0.9758 - val_loss: 0.1108\n",
      "Epoch 9/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9956 - loss: 0.0139 - val_accuracy: 0.9798 - val_loss: 0.0928\n",
      "Epoch 10/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9951 - loss: 0.0138 - val_accuracy: 0.9790 - val_loss: 0.1048\n",
      "Epoch 11/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9959 - loss: 0.0122 - val_accuracy: 0.9814 - val_loss: 0.0839\n",
      "Epoch 12/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 0.9810 - val_loss: 0.0976\n",
      "Epoch 13/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9963 - loss: 0.0110 - val_accuracy: 0.9832 - val_loss: 0.0923\n",
      "Epoch 14/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9974 - loss: 0.0080 - val_accuracy: 0.9774 - val_loss: 0.1176\n",
      "Epoch 15/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0093 - val_accuracy: 0.9812 - val_loss: 0.1108\n",
      "Epoch 16/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0098 - val_accuracy: 0.9782 - val_loss: 0.1279\n",
      "Epoch 17/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9968 - loss: 0.0098 - val_accuracy: 0.9826 - val_loss: 0.1334\n",
      "Epoch 18/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9968 - loss: 0.0115 - val_accuracy: 0.9802 - val_loss: 0.1360\n",
      "Epoch 19/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9967 - loss: 0.0121 - val_accuracy: 0.9810 - val_loss: 0.1233\n",
      "Epoch 20/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9971 - loss: 0.0087 - val_accuracy: 0.9840 - val_loss: 0.0998\n"
     ]
    }
   ],
   "source": [
    "MODEL_RANDOM = model_random.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=20,batch_size=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ccfb884-3856-45b0-b577-0e8bec94fa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9793 - loss: 0.1202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09846686571836472, 0.9824000000953674]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_random.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf45ba-aafa-4caf-bdce-1403b84e74a3",
   "metadata": {},
   "source": [
    "**Xavier Initializer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5572a4d2-fdb0-49d3-a933-6eadd74ee6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "XAVIER_LAYER = [\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(300,activation='relu',kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "    tf.keras.layers.Dense(110,activation='relu',kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=tf.keras.initializers.GlorotNormal()),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e048ac38-cf52-42a5-b271-a5c7c1622a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_model = tf.keras.models.Sequential(XAVIER_LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "971a6e1b-a66f-41ef-88d2-31c382eb9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1e79aef-a5d3-4c55-ae6e-828e45c2e7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8927 - loss: 0.3687 - val_accuracy: 0.9668 - val_loss: 0.1118\n",
      "Epoch 2/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9718 - loss: 0.0908 - val_accuracy: 0.9718 - val_loss: 0.0900\n",
      "Epoch 3/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9808 - loss: 0.0621 - val_accuracy: 0.9776 - val_loss: 0.0785\n",
      "Epoch 4/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9862 - loss: 0.0420 - val_accuracy: 0.9742 - val_loss: 0.0785\n",
      "Epoch 5/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9908 - loss: 0.0289 - val_accuracy: 0.9794 - val_loss: 0.0729\n",
      "Epoch 6/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9913 - loss: 0.0269 - val_accuracy: 0.9802 - val_loss: 0.0779\n",
      "Epoch 7/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9925 - loss: 0.0202 - val_accuracy: 0.9802 - val_loss: 0.0905\n",
      "Epoch 8/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9942 - loss: 0.0180 - val_accuracy: 0.9800 - val_loss: 0.0914\n",
      "Epoch 9/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9941 - loss: 0.0169 - val_accuracy: 0.9826 - val_loss: 0.0853\n",
      "Epoch 10/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9957 - loss: 0.0138 - val_accuracy: 0.9812 - val_loss: 0.0973\n",
      "Epoch 11/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9958 - loss: 0.0130 - val_accuracy: 0.9812 - val_loss: 0.0840\n",
      "Epoch 12/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9959 - loss: 0.0127 - val_accuracy: 0.9780 - val_loss: 0.0913\n",
      "Epoch 13/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9966 - loss: 0.0104 - val_accuracy: 0.9806 - val_loss: 0.0969\n",
      "Epoch 14/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9962 - loss: 0.0115 - val_accuracy: 0.9830 - val_loss: 0.0968\n",
      "Epoch 15/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9969 - loss: 0.0103 - val_accuracy: 0.9818 - val_loss: 0.1072\n",
      "Epoch 16/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9967 - loss: 0.0091 - val_accuracy: 0.9824 - val_loss: 0.1078\n",
      "Epoch 17/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9971 - loss: 0.0090 - val_accuracy: 0.9808 - val_loss: 0.1162\n",
      "Epoch 18/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9973 - loss: 0.0084 - val_accuracy: 0.9810 - val_loss: 0.1077\n",
      "Epoch 19/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9968 - loss: 0.0112 - val_accuracy: 0.9814 - val_loss: 0.1159\n",
      "Epoch 20/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0071 - val_accuracy: 0.9834 - val_loss: 0.1128\n"
     ]
    }
   ],
   "source": [
    "XAVIER_MODEL = xavier_model.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=20,batch_size=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fd4ac3f-8026-4224-8c4e-70ba3fe08172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9787 - loss: 0.1465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12370944768190384, 0.98089998960495]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xavier_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcb26a-28ac-4bfb-899b-e9a40ebf4ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e72eabb2-9e8a-4ba6-bb0e-9b10fa6465b6",
   "metadata": {},
   "source": [
    "**He Initializer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4aa0567-a7c1-42c1-b541-e8e09bc65284",
   "metadata": {},
   "outputs": [],
   "source": [
    "HE_INITIALIZER = [\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(300,activation='relu',kernel_initializer=tf.keras.initializers.HeNormal(),bias_initializer=tf.keras.initializers.HeNormal()),\n",
    "    tf.keras.layers.Dense(110,activation='relu',kernel_initializer=tf.keras.initializers.HeNormal(),bias_initializer=tf.keras.initializers.HeNormal()),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67153118-8118-4f6e-8220-52f25cccfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_model = tf.keras.models.Sequential(HE_INITIALIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44300fd9-34ee-4aec-8d06-d8cf07775a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3302019f-0e55-4ea4-a158-f68917e1c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8922 - loss: 0.3686 - val_accuracy: 0.9708 - val_loss: 0.0959\n",
      "Epoch 2/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9732 - loss: 0.0860 - val_accuracy: 0.9740 - val_loss: 0.0880\n",
      "Epoch 3/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9825 - loss: 0.0573 - val_accuracy: 0.9820 - val_loss: 0.0673\n",
      "Epoch 4/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0386 - val_accuracy: 0.9756 - val_loss: 0.0762\n",
      "Epoch 5/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9906 - loss: 0.0294 - val_accuracy: 0.9778 - val_loss: 0.0749\n",
      "Epoch 6/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0235 - val_accuracy: 0.9786 - val_loss: 0.0795\n",
      "Epoch 7/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9934 - loss: 0.0202 - val_accuracy: 0.9756 - val_loss: 0.0881\n",
      "Epoch 8/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9937 - loss: 0.0190 - val_accuracy: 0.9804 - val_loss: 0.0844\n",
      "Epoch 9/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9950 - loss: 0.0154 - val_accuracy: 0.9828 - val_loss: 0.0839\n",
      "Epoch 10/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9955 - loss: 0.0123 - val_accuracy: 0.9776 - val_loss: 0.1015\n",
      "Epoch 11/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9958 - loss: 0.0130 - val_accuracy: 0.9824 - val_loss: 0.0888\n",
      "Epoch 12/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9969 - loss: 0.0116 - val_accuracy: 0.9792 - val_loss: 0.0983\n",
      "Epoch 13/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9963 - loss: 0.0107 - val_accuracy: 0.9818 - val_loss: 0.1021\n",
      "Epoch 14/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9961 - loss: 0.0121 - val_accuracy: 0.9800 - val_loss: 0.0862\n",
      "Epoch 15/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9983 - loss: 0.0055 - val_accuracy: 0.9800 - val_loss: 0.1101\n",
      "Epoch 16/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9961 - loss: 0.0136 - val_accuracy: 0.9822 - val_loss: 0.1093\n",
      "Epoch 17/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9969 - loss: 0.0096 - val_accuracy: 0.9802 - val_loss: 0.1328\n",
      "Epoch 18/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9966 - loss: 0.0098 - val_accuracy: 0.9822 - val_loss: 0.1064\n",
      "Epoch 19/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 0.9788 - val_loss: 0.1510\n",
      "Epoch 20/20\n",
      "\u001b[1m1667/1667\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9963 - loss: 0.0126 - val_accuracy: 0.9806 - val_loss: 0.1344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7efd285c8280>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_model.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=20,batch_size=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33db4ba1-09e2-4097-8772-b36b8fe4815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9756 - loss: 0.1490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11944650858640671, 0.9793999791145325]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c722dc6-dc17-4957-ab0c-34eab5980dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f74a3a1-f506-4aa1-9c79-20f3464fedb3",
   "metadata": {},
   "source": [
    "**Accuracy of Zero Initializer = 11.60%**\n",
    "\n",
    "**Accuracy of Random Initializer = 97.93%**\n",
    "\n",
    "**Accuracy of Xavier Initializer = 97.87%**\n",
    "\n",
    "**Accuracy of He Initializer = 97.56%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b48465-001c-4b40-be06-3360237ce21c",
   "metadata": {},
   "source": [
    "## 9. Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique for a given neural network architecture and task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839b53f-47a9-41c7-8631-4c8323126222",
   "metadata": {},
   "source": [
    "* **Consideration**\n",
    "\n",
    "1. **Activation Functioin** : Different activation functions interacts differently with initialzer. So we should choose the appropraite initializer according to out activation function.\n",
    "\n",
    "2. **Network Depth** : Deeper networks can cause the problem of vanishing or exploding gradient, by using the initializer like xavier or he we can maintain the variance and scale of gradient throughout the network.\n",
    "\n",
    "3. **Types of Layers** : Convolutional layer and fully connected layer might have different optimal initializer.\n",
    "\n",
    "* **Tradeoff**\n",
    "\n",
    "1. **Stability and Convergence Spedd** : He initializer do a stable training of network with ReLU() activation function, on the other side it is slightly slower at convergence than Xavier initializer.\n",
    "\n",
    "2. **Bias-Variance Tradeoff** : Too small weights can lead to to slower convergence and too high weights makes the training process unstable, techniques like Xavier/He Initializer aims to maintain the variance over the many layers.\n",
    "\n",
    "3. **Computational Efficiency** : Orthogonal initializer is more computationally intensive than standard Xavier/He Initializer, but this initializer is optimal for only some types of architecture.\n",
    "\n",
    "4. **Overfitting and Underfitting** : Poor initialization can lead underfitting, where network fails to understand underlying patterns. On the other side setting the weights too optimistically can lead overfitting, where model learn the training data well but fails to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcda566-072e-4ff8-9cdb-23d693e0c30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
